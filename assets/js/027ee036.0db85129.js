"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[4053],{5016:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Module-4-VLA/week-12-multimodal-interactions","title":"Week 12: Multi-modal Interactions","description":"Integrate vision-language models with robotic actions for natural human-robot collaboration through speech, gestures, and visual understanding.","source":"@site/docs/Module-4-VLA/week-12-multimodal-interactions.mdx","sourceDirName":"Module-4-VLA","slug":"/Module-4-VLA/week-12-multimodal-interactions","permalink":"/ai-robotics-book/docs/Module-4-VLA/week-12-multimodal-interactions","draft":false,"unlisted":false,"editUrl":"https://github.com/ai-robotics/ai-robotics-book/tree/main/docs/Module-4-VLA/week-12-multimodal-interactions.mdx","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"sidebar_position":12,"title":"Week 12: Multi-modal Interactions","description":"Integrate vision-language models with robotic actions for natural human-robot collaboration through speech, gestures, and visual understanding."},"sidebar":"tutorialSidebar","previous":{"title":"Week 11: Humanoid Robot Development","permalink":"/ai-robotics-book/docs/Module-4-VLA/week-11-humanoid-development"},"next":{"title":"Week 13: Conversational Robotics + Capstone Project","permalink":"/ai-robotics-book/docs/Module-4-VLA/week-13-conversational-robotics-capstone"}}');var s=i(4848),o=i(8453);const a={sidebar_position:12,title:"Week 12: Multi-modal Interactions",description:"Integrate vision-language models with robotic actions for natural human-robot collaboration through speech, gestures, and visual understanding."},r="Week 12: Multi-modal Interactions",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Vision-Language Models",id:"vision-language-models",level:2},{value:"CLIP: Connecting Vision and Language",id:"clip-connecting-vision-and-language",level:3},{value:"Code Example: CLIP for Object Detection",id:"code-example-clip-for-object-detection",level:3},{value:"GPT-4 Vision for Scene Understanding",id:"gpt-4-vision-for-scene-understanding",level:3},{value:"Speech Recognition with Whisper",id:"speech-recognition-with-whisper",level:2},{value:"OpenAI Whisper Integration",id:"openai-whisper-integration",level:3},{value:"Wake Word Detection",id:"wake-word-detection",level:3},{value:"Gesture Recognition",id:"gesture-recognition",level:2},{value:"MediaPipe Hand Tracking",id:"mediapipe-hand-tracking",level:3},{value:"VLA Pipeline: Language to Action",id:"vla-pipeline-language-to-action",level:2},{value:"Intent Recognition",id:"intent-recognition",level:3},{value:"LLM-Based Action Planning",id:"llm-based-action-planning",level:3},{value:"Multi-Modal Fusion",id:"multi-modal-fusion",level:2},{value:"Combining Vision, Speech, and Gesture",id:"combining-vision-speech-and-gesture",level:3},{value:"Self-Assessment Questions",id:"self-assessment-questions",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components},{Details:i}=n;return i||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"week-12-multi-modal-interactions",children:"Week 12: Multi-modal Interactions"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(n.p,{children:["The future of human-robot interaction isn't keyboards and buttons\u2014it's natural conversation, gestures, and visual understanding. ",(0,s.jsx)("span",{className:"highlight-purple",children:(0,s.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"})}),' systems enable robots to understand: "Pick up the red cup on the left table and bring it to me."']}),"\n",(0,s.jsx)(n.p,{children:"This week integrates three modalities:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vision"}),": Scene understanding, object detection, pose estimation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language"}),": Natural language processing, intent recognition"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action"}),": Translating high-level commands to robot motions"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"You'll implement VLA pipelines using state-of-the-art models (CLIP, Whisper, GPT-4 Vision) and deploy them on humanoid robots for intuitive interaction."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integrate"})," vision-language models (CLIP, GPT-4 Vision) with ROS 2"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement"})," speech recognition (Whisper) for voice commands"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deploy"})," gesture recognition for non-verbal interaction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Create"})," VLA pipelines that map language to robotic actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimize"})," multi-modal fusion for real-time performance"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"vision-language-models",children:"Vision-Language Models"}),"\n",(0,s.jsx)(n.h3,{id:"clip-connecting-vision-and-language",children:"CLIP: Connecting Vision and Language"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"CLIP (Contrastive Language-Image Pretraining)"})," learns to match images with text descriptions. Use cases:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'Zero-shot object detection: "Find the stapler" without training on staplers'}),"\n",(0,s.jsx)(n.li,{children:'Scene understanding: "Is this a kitchen or bedroom?"'}),"\n",(0,s.jsxs)(n.li,{children:['Visual grounding: "The cup ',(0,s.jsx)(n.strong,{children:"on the left"}),'"']}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:'flowchart LR\n    subgraph Input\n        Speech["User Speech<br/>\'Pick up the red cup\'"]\n        Camera["Camera<br/>(RGB Image)"]\n    end\n\n    subgraph Processing\n        Whisper["Whisper<br/>(Speech\u2192Text)"]\n        LLM["GPT-4<br/>(Intent Parser)"]\n        CLIP["CLIP<br/>(Visual Grounding)"]\n    end\n\n    subgraph Output\n        Action["Robot Action<br/>(Navigation + Grasp)"]\n    end\n\n    Speech --\x3e Whisper\n    Whisper --\x3e LLM\n    Camera --\x3e CLIP\n    LLM --\x3e|"object: cup<br/>color: red"| CLIP\n    CLIP --\x3e|"bounding box<br/>(x, y, w, h)"| Action\n\n    style LLM fill:#a855f7,stroke:#9333ea,stroke-width:2px,color:#fff\n    style CLIP fill:#ec4899,stroke:#db2777,stroke-width:2px,color:#fff\n    style Whisper fill:#06b6d4,stroke:#0891b2,stroke-width:2px,color:#fff\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Diagram:"})," Vision-Language-Action pipeline combining speech recognition (Whisper), language understanding (GPT-4), and visual grounding (CLIP) to execute robotic tasks."]}),"\n",(0,s.jsx)(n.h3,{id:"code-example-clip-for-object-detection",children:"Code Example: CLIP for Object Detection"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport torch\nimport clip\nfrom PIL import Image as PILImage\nimport numpy as np\n\n\nclass CLIPDetector(Node):\n    def __init__(self):\n        super().__init__(\'clip_detector\')\n\n        # Load CLIP model\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)\n\n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10\n        )\n\n        self.bridge = CvBridge()\n\n        # Target objects to detect\n        self.text_prompts = ["a red cup", "a blue bottle", "a laptop", "a book"]\n        self.text_tokens = clip.tokenize(self.text_prompts).to(self.device)\n\n    def image_callback(self, msg):\n        """Detect objects in image using CLIP"""\n\n        # Convert ROS Image to PIL\n        cv_image = self.bridge.imgmsg_to_cv2(msg, "rgb8")\n        pil_image = PILImage.fromarray(cv_image)\n\n        # Preprocess for CLIP\n        image_input = self.preprocess(pil_image).unsqueeze(0).to(self.device)\n\n        # Compute similarity scores\n        with torch.no_grad():\n            image_features = self.model.encode_image(image_input)\n            text_features = self.model.encode_text(self.text_tokens)\n\n            # Cosine similarity\n            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\n        # Get top match\n        values, indices = similarity[0].topk(1)\n        top_match = self.text_prompts[indices[0]]\n        confidence = values[0].item()\n\n        self.get_logger().info(\n            f\'Detected: {top_match} (confidence: {confidence:.2%})\'\n        )\n\n\ndef main():\n    rclpy.init()\n    detector = CLIPDetector()\n    rclpy.spin(detector)\n    detector.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"gpt-4-vision-for-scene-understanding",children:"GPT-4 Vision for Scene Understanding"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nimport base64\n\nclass GPT4VisionNode(Node):\n    def __init__(self):\n        super().__init__(\'gpt4_vision\')\n        openai.api_key = "YOUR_API_KEY"\n\n    def analyze_scene(self, image_path):\n        """Ask GPT-4 Vision about the scene"""\n\n        # Encode image to base64\n        with open(image_path, "rb") as image_file:\n            base64_image = base64.b64encode(image_file.read()).decode(\'utf-8\')\n\n        response = openai.ChatCompletion.create(\n            model="gpt-4-vision-preview",\n            messages=[\n                {\n                    "role": "user",\n                    "content": [\n                        {"type": "text", "text": "Describe this scene for a robot. List objects and their locations."},\n                        {\n                            "type": "image_url",\n                            "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}\n                        }\n                    ]\n                }\n            ],\n            max_tokens=300\n        )\n\n        description = response.choices[0].message.content\n        self.get_logger().info(f\'Scene: {description}\')\n        return description\n'})}),"\n",(0,s.jsx)(n.h2,{id:"speech-recognition-with-whisper",children:"Speech Recognition with Whisper"}),"\n",(0,s.jsx)(n.h3,{id:"openai-whisper-integration",children:"OpenAI Whisper Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport whisper\nimport sounddevice as sd\nimport numpy as np\nimport tempfile\nimport wave\n\n\nclass WhisperSpeechRecognition(Node):\n    def __init__(self):\n        super().__init__('whisper_speech')\n\n        # Load Whisper model (base = good balance of speed/accuracy)\n        self.model = whisper.load_model(\"base\")\n\n        # Publisher for transcribed text\n        self.text_pub = self.create_publisher(String, '/speech/text', 10)\n\n        # Parameters\n        self.sample_rate = 16000  # Whisper expects 16kHz\n        self.duration = 3  # Record 3-second chunks\n\n        # Start listening\n        self.create_timer(3.0, self.record_and_transcribe)\n\n        self.get_logger().info('Whisper speech recognition started')\n\n    def record_and_transcribe(self):\n        \"\"\"Record audio and transcribe with Whisper\"\"\"\n\n        # Record audio\n        self.get_logger().info('Listening...')\n        audio = sd.rec(\n            int(self.duration * self.sample_rate),\n            samplerate=self.sample_rate,\n            channels=1,\n            dtype='int16'\n        )\n        sd.wait()\n\n        # Save to temporary file\n        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as f:\n            with wave.open(f.name, 'wb') as wf:\n                wf.setnchannels(1)\n                wf.setsampwidth(2)  # 16-bit\n                wf.setframerate(self.sample_rate)\n                wf.writeframes(audio.tobytes())\n\n            # Transcribe\n            result = self.model.transcribe(f.name, language='en')\n            text = result[\"text\"].strip()\n\n            if text:\n                self.get_logger().info(f'Heard: \"{text}\"')\n\n                # Publish transcription\n                msg = String()\n                msg.data = text\n                self.text_pub.publish(msg)\n\n\ndef main():\n    rclpy.init()\n    node = WhisperSpeechRecognition()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"wake-word-detection",children:"Wake Word Detection"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import pvporcupine\n\nclass WakeWordDetector(Node):\n    def __init__(self):\n        super().__init__(\'wake_word\')\n\n        # Initialize Porcupine (wake word engine)\n        self.porcupine = pvporcupine.create(\n            keywords=["hey robot"]  # Custom wake word\n        )\n\n        self.audio_stream = sd.InputStream(\n            samplerate=self.porcupine.sample_rate,\n            channels=1,\n            callback=self.audio_callback\n        )\n        self.audio_stream.start()\n\n    def audio_callback(self, indata, frames, time, status):\n        """Check for wake word"""\n\n        # Convert to int16\n        pcm = np.frombuffer(indata, dtype=np.int16)\n\n        keyword_index = self.porcupine.process(pcm)\n\n        if keyword_index >= 0:\n            self.get_logger().info(\'Wake word detected!\')\n            # Start full speech recognition\n'})}),"\n",(0,s.jsx)(n.h2,{id:"gesture-recognition",children:"Gesture Recognition"}),"\n",(0,s.jsx)(n.h3,{id:"mediapipe-hand-tracking",children:"MediaPipe Hand Tracking"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import mediapipe as mp\nfrom mediapipe.tasks import python\nfrom mediapipe.tasks.python import vision\n\n\nclass GestureRecognizer(Node):\n    def __init__(self):\n        super().__init__(\'gesture_recognizer\')\n\n        # Load MediaPipe gesture recognizer\n        base_options = python.BaseOptions(\n            model_asset_path=\'gesture_recognizer.task\'\n        )\n        options = vision.GestureRecognizerOptions(\n            base_options=base_options,\n            running_mode=vision.RunningMode.VIDEO\n        )\n        self.recognizer = vision.GestureRecognizer.create_from_options(options)\n\n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10\n        )\n\n        self.bridge = CvBridge()\n\n    def image_callback(self, msg):\n        """Recognize gestures in image"""\n\n        # Convert to MediaPipe format\n        cv_image = self.bridge.imgmsg_to_cv2(msg, "rgb8")\n        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv_image)\n\n        # Recognize\n        result = self.recognizer.recognize(mp_image)\n\n        if result.gestures:\n            top_gesture = result.gestures[0][0]\n            self.get_logger().info(\n                f\'Gesture: {top_gesture.category_name} \'\n                f\'(confidence: {top_gesture.score:.2%})\'\n            )\n\n            # Map gesture to command\n            self.execute_gesture_command(top_gesture.category_name)\n\n    def execute_gesture_command(self, gesture):\n        """Map gesture to robot action"""\n\n        gesture_map = {\n            "Thumb_Up": "confirm_action",\n            "Open_Palm": "stop",\n            "Pointing_Up": "move_forward",\n            "Victory": "take_photo",\n        }\n\n        if gesture in gesture_map:\n            command = gesture_map[gesture]\n            self.get_logger().info(f\'Executing: {command}\')\n            # Publish command to action server\n'})}),"\n",(0,s.jsx)(n.h2,{id:"vla-pipeline-language-to-action",children:"VLA Pipeline: Language to Action"}),"\n",(0,s.jsx)(n.h3,{id:"intent-recognition",children:"Intent Recognition"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class IntentParser(Node):\n    def __init__(self):\n        super().__init__(\'intent_parser\')\n\n        # Subscribe to speech transcriptions\n        self.speech_sub = self.create_subscription(\n            String, \'/speech/text\', self.parse_intent, 10\n        )\n\n        # Action client for robot commands\n        self.nav_client = ActionClient(self, NavigateToPose, \'/navigate_to_pose\')\n\n    def parse_intent(self, msg):\n        """Extract intent and entities from speech"""\n\n        text = msg.data.lower()\n\n        # Simple keyword-based parsing (use LLM for production)\n        if "pick up" in text or "grab" in text:\n            # Extract object\n            object_name = self.extract_object(text)\n            self.execute_pick_task(object_name)\n\n        elif "go to" in text or "navigate to" in text:\n            location = self.extract_location(text)\n            self.execute_navigation(location)\n\n        elif "bring me" in text:\n            object_name = self.extract_object(text)\n            self.execute_fetch_task(object_name)\n\n    def extract_object(self, text):\n        """Extract object name from text"""\n        # Use spaCy NER or GPT for better extraction\n        objects = ["cup", "bottle", "book", "phone"]\n        for obj in objects:\n            if obj in text:\n                return obj\n        return None\n\n    def execute_pick_task(self, object_name):\n        """Execute pick-and-place task"""\n        self.get_logger().info(f\'Picking up {object_name}...\')\n        # 1. Detect object with CLIP\n        # 2. Plan grasp with cuMotion\n        # 3. Execute\n'})}),"\n",(0,s.jsx)(n.h3,{id:"llm-based-action-planning",children:"LLM-Based Action Planning"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def plan_task_with_llm(self, user_command):\n    """Use GPT-4 to decompose command into action sequence"""\n\n    prompt = f"""\nYou are a robot assistant. Break down this command into a sequence of primitive actions.\n\nAvailable actions:\n- navigate_to(location)\n- detect_object(object_name)\n- grasp_object()\n- place_object(location)\n- say(text)\n\nUser command: "{user_command}"\n\nOutput format (JSON):\n{{"actions": [{{"type": "navigate_to", "params": {{"location": "kitchen"}}}}, ...]}}\n"""\n\n    response = openai.ChatCompletion.create(\n        model="gpt-4",\n        messages=[{"role": "user", "content": prompt}],\n        max_tokens=200\n    )\n\n    # Parse JSON response\n    import json\n    plan = json.loads(response.choices[0].message.content)\n\n    return plan["actions"]\n'})}),"\n",(0,s.jsx)(n.h2,{id:"multi-modal-fusion",children:"Multi-Modal Fusion"}),"\n",(0,s.jsx)(n.h3,{id:"combining-vision-speech-and-gesture",children:"Combining Vision, Speech, and Gesture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class MultiModalFusion(Node):\n    def __init__(self):\n        super().__init__('multimodal_fusion')\n\n        # Inputs\n        self.speech_sub = self.create_subscription(String, '/speech/text', self.on_speech, 10)\n        self.gesture_sub = self.create_subscription(String, '/gesture', self.on_gesture, 10)\n        self.vision_sub = self.create_subscription(String, '/detected_objects', self.on_vision, 10)\n\n        # State\n        self.last_speech = None\n        self.last_gesture = None\n        self.detected_objects = []\n\n    def on_speech(self, msg):\n        self.last_speech = msg.data\n\n        # Check if gesture and speech combined provide complete command\n        if self.last_gesture and \"that\" in self.last_speech:\n            # \"Pick up that\" + pointing gesture\n            self.execute_multimodal_command()\n\n    def execute_multimodal_command(self):\n        \"\"\"Resolve 'that' pronoun using gesture pointing direction\"\"\"\n\n        # Use gesture to determine which object\n        # (In production, raycast from hand direction to find nearest object)\n        target_object = self.detected_objects[0]  # Simplified\n\n        self.get_logger().info(\n            f'Multimodal command: {self.last_speech} \u2192 Targeting {target_object}'\n        )\n"})}),"\n",(0,s.jsx)(n.h2,{id:"self-assessment-questions",children:"Self-Assessment Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"How does CLIP enable zero-shot object detection?"})}),"\n",(0,s.jsx)(i,{children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)("summary",{children:"Answer"}),'\nCLIP learns a shared embedding space for images and text during pretraining on 400M image-text pairs. At inference, you provide text descriptions (e.g., "a red cup") without training on that specific object class. CLIP computes cosine similarity between image embeddings and text embeddings\u2014highest similarity indicates the match. This eliminates the need for object-specific training datasets. However, accuracy is lower than supervised detectors (e.g., YOLO trained on cups), making CLIP best for flexible, open-vocabulary detection where training data is unavailable.']})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why is Whisper better than traditional speech recognition for robotics?"})}),"\n",(0,s.jsx)(i,{children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)("summary",{children:"Answer"}),"\nWhisper is trained on 680,000 hours of multilingual data with robust noise handling, making it work in real-world environments (background noise, accents, varying audio quality) where traditional ASR fails. It supports 99 languages out-of-the-box, critical for global robot deployment. Whisper also handles non-standard speech (stammering, code-switching) better. Additionally, it's open-source and runs locally (no cloud dependency), which is essential for privacy and low-latency robot interaction."]})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"What is the advantage of using an LLM (GPT-4) for action planning instead of hardcoded rules?"})}),"\n",(0,s.jsx)(i,{children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)("summary",{children:"Answer"}),'\nLLMs handle ambiguous, complex commands that hardcoded rules can\'t parse: "Tidy up the living room"\u2014what does "tidy" mean? An LLM can decompose this into: (1) detect misplaced objects, (2) classify object types, (3) infer target locations (books \u2192 shelf), (4) plan pick-and-place sequences. Hardcoded rules would require manually encoding every variation of "tidy," "clean up," "organize." LLMs also generalize to novel commands via in-context learning, whereas rules require explicit programming for each new task.']})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why combine gesture and speech recognition instead of using only one modality?"})}),"\n",(0,s.jsx)(i,{children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)("summary",{children:"Answer"}),'\nMultimodal input is more robust and natural. Speech alone fails with pronouns ("pick up that") without pointing. Gestures alone are ambiguous (pointing could mean "go there" or "pick that up"). Combining modalities resolves ambiguity: speech provides intent, gesture specifies target. This mirrors human communication\u2014we naturally point while saying "that cup." Additionally, multimodal systems handle partial failures: if speech recognition fails in a noisy environment, gestures provide fallback. Fusion improves accuracy and user experience.']})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"What latency challenges arise when deploying VLA pipelines on humanoid robots?"})}),"\n",(0,s.jsx)(i,{children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)("summary",{children:"Answer"}),"\nVLA pipelines involve sequential processing: (1) speech recognition (Whisper: 0.5-2s for 3s audio), (2) LLM intent parsing (GPT-4: 1-3s API latency), (3) vision processing (CLIP: 50-100ms), (4) motion planning (cuMotion: 100-500ms). Total: 2-6 seconds from speech to action start\u2014unacceptably slow for natural interaction. Optimizations: (1) Use smaller local models (GPT-2 instead of GPT-4), (2) Cache common commands, (3) Pipeline parallelism (start vision processing during speech transcription), (4) Predictive action (prepare likely actions during user speech). Target: <1s total latency for fluid interaction."]})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vision-language models"})," (CLIP, GPT-4 Vision) enable flexible object understanding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Whisper"})," provides robust speech recognition in noisy environments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gesture recognition"})," (MediaPipe) allows non-verbal interaction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"VLA pipelines"})," translate natural language to robot actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multimodal fusion"})," combines modalities for robust, intuitive commands"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.p,{children:["Week 13: ",(0,s.jsx)(n.strong,{children:"Conversational Robotics + Capstone Project"}),"\u2014build an end-to-end VLA system integrating everything from this course, culminating in a final demo of a humanoid robot responding to natural voice commands, navigating autonomously, and performing complex manipulation tasks."]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var t=i(6540);const s={},o=t.createContext(s);function a(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);