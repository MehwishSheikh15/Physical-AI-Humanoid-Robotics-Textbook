"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[4529],{1759:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"Module-4-VLA/week-13-conversational-robotics-capstone","title":"Week 13: Conversational Robotics + Capstone Project","description":"Integrate all course concepts into a complete VLA system: conversational AI, autonomous navigation, manipulation, and human-robot collaboration.","source":"@site/docs/Module-4-VLA/week-13-conversational-robotics-capstone.mdx","sourceDirName":"Module-4-VLA","slug":"/Module-4-VLA/week-13-conversational-robotics-capstone","permalink":"/ai-robotics-book/docs/Module-4-VLA/week-13-conversational-robotics-capstone","draft":false,"unlisted":false,"editUrl":"https://github.com/ai-robotics/ai-robotics-book/tree/main/docs/Module-4-VLA/week-13-conversational-robotics-capstone.mdx","tags":[],"version":"current","sidebarPosition":13,"frontMatter":{"sidebar_position":13,"title":"Week 13: Conversational Robotics + Capstone Project","description":"Integrate all course concepts into a complete VLA system: conversational AI, autonomous navigation, manipulation, and human-robot collaboration."},"sidebar":"tutorialSidebar","previous":{"title":"Week 12: Multi-modal Interactions","permalink":"/ai-robotics-book/docs/Module-4-VLA/week-12-multimodal-interactions"}}');var i=t(4848),o=t(8453);const a={sidebar_position:13,title:"Week 13: Conversational Robotics + Capstone Project",description:"Integrate all course concepts into a complete VLA system: conversational AI, autonomous navigation, manipulation, and human-robot collaboration."},r="Week 13: Conversational Robotics + Capstone Project",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Capstone Project Specification",id:"capstone-project-specification",level:2},{value:"Project Goal",id:"project-goal",level:3},{value:"Required Capabilities",id:"required-capabilities",level:3},{value:"Example Task",id:"example-task",level:3},{value:"System Architecture",id:"system-architecture",level:2},{value:"High-Level Architecture",id:"high-level-architecture",level:3},{value:"Data Flow",id:"data-flow",level:3},{value:"Implementation: Core Components",id:"implementation-core-components",level:2},{value:"1. Dialogue Manager",id:"1-dialogue-manager",level:3},{value:"2. Task Executor (Behavior Tree)",id:"2-task-executor-behavior-tree",level:3},{value:"3. Text-to-Speech Integration",id:"3-text-to-speech-integration",level:3},{value:"4. Integration Launch File",id:"4-integration-launch-file",level:3},{value:"Testing Strategy",id:"testing-strategy",level:2},{value:"Phase 1: Simulation Testing",id:"phase-1-simulation-testing",level:3},{value:"Phase 2: Hardware Deployment",id:"phase-2-hardware-deployment",level:3},{value:"Evaluation Criteria",id:"evaluation-criteria",level:2},{value:"Functionality (60 points)",id:"functionality-60-points",level:3},{value:"Robustness (20 points)",id:"robustness-20-points",level:3},{value:"Technical Quality (10 points)",id:"technical-quality-10-points",level:3},{value:"Presentation (10 points)",id:"presentation-10-points",level:3},{value:"Common Challenges &amp; Solutions",id:"common-challenges--solutions",level:2},{value:"Challenge 1: Speech Recognition in Noisy Environments",id:"challenge-1-speech-recognition-in-noisy-environments",level:3},{value:"Challenge 2: Object Detection False Positives",id:"challenge-2-object-detection-false-positives",level:3},{value:"Challenge 3: Manipulation Collisions",id:"challenge-3-manipulation-collisions",level:3},{value:"Challenge 4: Latency in VLA Pipeline",id:"challenge-4-latency-in-vla-pipeline",level:3},{value:"Extensions &amp; Advanced Features",id:"extensions--advanced-features",level:2},{value:"Self-Assessment Questions",id:"self-assessment-questions",level:2},{value:"Summary",id:"summary",level:2},{value:"Congratulations!",id:"congratulations",level:2},{value:"Next Steps in Your Journey",id:"next-steps-in-your-journey",level:3},{value:"Capstone Submission Checklist",id:"capstone-submission-checklist",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"week-13-conversational-robotics--capstone-project",children:"Week 13: Conversational Robotics + Capstone Project"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(n.p,{children:["Welcome to the culmination of your journey into Physical AI and Humanoid Robotics. Over 12 weeks, you've mastered ROS 2, simulation, Isaac perception, and vision-language-action systems. This final week brings it all together in a ",(0,i.jsx)(n.strong,{children:"capstone project"}),": a fully autonomous humanoid robot that understands natural language, navigates environments, manipulates objects, and collaborates with humans."]}),"\n",(0,i.jsx)(n.p,{children:'You\'ll build a conversational robotics system where users interact naturally: "Hey robot, bring me the blue bottle from the kitchen table." The robot will parse the command, plan a multi-step task, execute navigation and manipulation, and provide verbal feedback\u2014all autonomously.'}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integrate"})," all course modules (ROS 2, Gazebo, Isaac, VLA) into one system"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implement"})," a conversational AI agent with context-aware dialogue management"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Deploy"})," end-to-end task execution from speech input to physical action"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Demonstrate"})," robust error handling and recovery behaviors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Document"})," system architecture and present project outcomes"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"capstone-project-specification",children:"Capstone Project Specification"}),"\n",(0,i.jsx)(n.h3,{id:"project-goal",children:"Project Goal"}),"\n",(0,i.jsxs)(n.p,{children:["Build an ",(0,i.jsx)(n.strong,{children:"Intelligent Fetch Robot"})," that:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Accepts natural language commands via speech"}),"\n",(0,i.jsx)(n.li,{children:"Understands object references and spatial relationships"}),"\n",(0,i.jsx)(n.li,{children:"Navigates to target locations autonomously"}),"\n",(0,i.jsx)(n.li,{children:"Manipulates objects (pick and place)"}),"\n",(0,i.jsx)(n.li,{children:"Provides spoken feedback during task execution"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"required-capabilities",children:"Required Capabilities"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Capability"}),(0,i.jsx)(n.th,{children:"Module Used"}),(0,i.jsx)(n.th,{children:"Implementation"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Speech Input"})}),(0,i.jsx)(n.td,{children:"VLA (Week 12)"}),(0,i.jsx)(n.td,{children:"Whisper + wake word detection"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Language Understanding"})}),(0,i.jsx)(n.td,{children:"VLA (Week 12)"}),(0,i.jsx)(n.td,{children:"GPT-4 for intent parsing"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Visual Perception"})}),(0,i.jsx)(n.td,{children:"Isaac (Week 9)"}),(0,i.jsx)(n.td,{children:"CLIP for object detection"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Localization"})}),(0,i.jsx)(n.td,{children:"Isaac (Week 9)"}),(0,i.jsx)(n.td,{children:"cuVSLAM for mapping"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Navigation"})}),(0,i.jsx)(n.td,{children:"Isaac (Week 10)"}),(0,i.jsx)(n.td,{children:"Nav2 with behavior trees"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Manipulation"})}),(0,i.jsx)(n.td,{children:"Isaac (Week 10)"}),(0,i.jsx)(n.td,{children:"cuMotion for motion planning"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Simulation"})}),(0,i.jsx)(n.td,{children:"Gazebo (Weeks 6-7)"}),(0,i.jsx)(n.td,{children:"Test in virtual environment first"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Real Hardware"})}),(0,i.jsx)(n.td,{children:"Humanoids (Week 11)"}),(0,i.jsx)(n.td,{children:"Deploy on Unitree G1 / simulation"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"example-task",children:"Example Task"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"User"}),': "Hey robot, bring me the red cup from the kitchen counter."']}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Robot Execution"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Parse"}),": Identify action (fetch), object (red cup), location (kitchen counter), recipient (user)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Plan"}),": Navigate to kitchen \u2192 Detect red cup \u2192 Grasp \u2192 Navigate to user \u2192 Hand over"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Execute"}),": Run Nav2 navigation \u2192 Use CLIP to find cup \u2192 Execute grasp with cuMotion \u2192 Return"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback"}),': "I\'m heading to the kitchen... I found the red cup... Bringing it to you now... Here you go!"']}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"high-level-architecture",children:"High-Level Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:'graph TB\n    subgraph "Conversational AI Layer"\n        Speech["Speech I/O<br/>(Whisper, TTS)"]\n        Dialogue["Dialogue Manager<br/>(GPT-4)"]\n        Intent["Intent Parser<br/>(Task Understanding)"]\n    end\n\n    subgraph "Task Planning Layer"\n        BT["Behavior Trees<br/>(Action Sequencing)"]\n        STM["State Machine<br/>(Task Monitoring)"]\n    end\n\n    subgraph "Execution Layer"\n        Perception["Perception<br/>(Isaac ROS)"]\n        Navigation["Navigation<br/>(Nav2)"]\n        Manipulation["Manipulation<br/>(cuMotion)"]\n    end\n\n    Hardware["Robot Hardware<br/>(Unitree G1 / Sim)"]\n\n    Speech --\x3e Dialogue\n    Dialogue --\x3e Intent\n    Intent --\x3e BT\n    BT --\x3e STM\n    STM --\x3e Perception\n    STM --\x3e Navigation\n    STM --\x3e Manipulation\n    Perception --\x3e Hardware\n    Navigation --\x3e Hardware\n    Manipulation --\x3e Hardware\n    Hardware --\x3e STM\n\n    style Dialogue fill:#a855f7,stroke:#9333ea,stroke-width:2px,color:#fff\n    style BT fill:#ec4899,stroke:#db2777,stroke-width:2px,color:#fff\n    style Hardware fill:#06b6d4,stroke:#0891b2,stroke-width:2px,color:#fff\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Diagram:"})," Complete conversational robotics system architecture integrating speech, dialogue management, task planning, perception, navigation, and manipulation into a unified autonomous system."]}),"\n",(0,i.jsx)(n.h3,{id:"data-flow",children:"Data Flow"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"User Speech \u2192 Whisper \u2192 Text \u2192 GPT-4 \u2192 Task Plan \u2192 Behavior Tree\n                                                          \u2193\n                                         Perception \u2190 Camera/LIDAR\n                                              \u2193\n                                    Object Detection (CLIP)\n                                              \u2193\n                                         Navigation\n                                              \u2193\n                                         Manipulation\n                                              \u2193\n                                      Status \u2192 TTS \u2192 User\n"})}),"\n",(0,i.jsx)(n.h2,{id:"implementation-core-components",children:"Implementation: Core Components"}),"\n",(0,i.jsx)(n.h3,{id:"1-dialogue-manager",children:"1. Dialogue Manager"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport openai\n\n\nclass DialogueManager(Node):\n    def __init__(self):\n        super().__init__(\'dialogue_manager\')\n\n        openai.api_key = "YOUR_KEY"\n\n        # Conversation history\n        self.conversation_history = [\n            {"role": "system", "content": """You are a helpful robot assistant.\n            You can navigate, pick up objects, and perform household tasks.\n            Parse user commands into structured task plans."""}\n        ]\n\n        # Subscribers\n        self.speech_sub = self.create_subscription(\n            String, \'/speech/text\', self.on_user_speech, 10\n        )\n\n        # Publishers\n        self.task_plan_pub = self.create_publisher(String, \'/task_plan\', 10)\n        self.tts_pub = self.create_publisher(String, \'/robot/speech\', 10)\n\n        self.get_logger().info(\'Dialogue Manager ready\')\n\n    def on_user_speech(self, msg):\n        """Process user speech and generate task plan"""\n\n        user_input = msg.data\n        self.get_logger().info(f\'User: {user_input}\')\n\n        # Add to conversation\n        self.conversation_history.append(\n            {"role": "user", "content": user_input}\n        )\n\n        # Get LLM response\n        response = openai.ChatCompletion.create(\n            model="gpt-4",\n            messages=self.conversation_history,\n            max_tokens=200\n        )\n\n        assistant_message = response.choices[0].message.content\n\n        # Add to history\n        self.conversation_history.append(\n            {"role": "assistant", "content": assistant_message}\n        )\n\n        self.get_logger().info(f\'Robot: {assistant_message}\')\n\n        # Publish task plan\n        task_msg = String()\n        task_msg.data = assistant_message\n        self.task_plan_pub.publish(task_msg)\n\n        # Speak response\n        self.speak(assistant_message)\n\n    def speak(self, text):\n        """Send text to TTS system"""\n        tts_msg = String()\n        tts_msg.data = text\n        self.tts_pub.publish(tts_msg)\n\n\ndef main():\n    rclpy.init()\n    manager = DialogueManager()\n    rclpy.spin(manager)\n    manager.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-task-executor-behavior-tree",children:"2. Task Executor (Behavior Tree)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- fetch_object.xml --\x3e\n\n<root main_tree_to_execute="FetchObjectTree">\n  <BehaviorTree ID="FetchObjectTree">\n    <Sequence name="FetchSequence">\n\n      \x3c!-- Step 1: Navigate to object location --\x3e\n      <Action ID="SpeakText" text="I\'m heading to the {location}"/>\n\n      <Action ID="NavigateToLocation"\n              location="{location}"\n              server_name="navigate_to_pose"/>\n\n      \x3c!-- Step 2: Detect object using vision --\x3e\n      <Action ID="SpeakText" text="Searching for {object_name}"/>\n\n      <Action ID="DetectObject"\n              object_name="{object_name}"\n              detection_method="CLIP"/>\n\n      \x3c!-- Step 3: Grasp object --\x3e\n      <Action ID="SpeakText" text="I found the {object_name}. Grasping it now."/>\n\n      <Action ID="PlanGrasp"\n              object_id="{detected_object_id}"\n              planner="cuMotion"/>\n\n      <Action ID="ExecuteGrasp"/>\n\n      \x3c!-- Step 4: Navigate to user --\x3e\n      <Action ID="SpeakText" text="Bringing it to you"/>\n\n      <Action ID="NavigateToLocation"\n              location="user_location"\n              server_name="navigate_to_pose"/>\n\n      \x3c!-- Step 5: Hand over --\x3e\n      <Action ID="SpeakText" text="Here you go!"/>\n\n      <Action ID="ReleaseGripper"/>\n\n      \x3c!-- Success --\x3e\n      <Action ID="SpeakText" text="Task completed!"/>\n\n    </Sequence>\n  </BehaviorTree>\n\n  \x3c!-- Recovery behaviors --\x3e\n  <BehaviorTree ID="RecoveryTree">\n    <Fallback name="Recovery">\n      <Action ID="Spin" angle="360"/>\n      <Action ID="Backup" distance="0.5"/>\n      <Action ID="SpeakText" text="I need help. Please assist me."/>\n    </Fallback>\n  </BehaviorTree>\n</root>\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3-text-to-speech-integration",children:"3. Text-to-Speech Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from gtts import gTTS\nimport pygame\nimport tempfile\n\n\nclass TextToSpeech(Node):\n    def __init__(self):\n        super().__init__('tts')\n\n        pygame.mixer.init()\n\n        self.speech_sub = self.create_subscription(\n            String, '/robot/speech', self.speak, 10\n        )\n\n    def speak(self, msg):\n        \"\"\"Convert text to speech and play\"\"\"\n\n        text = msg.data\n\n        # Generate speech\n        tts = gTTS(text=text, lang='en', slow=False)\n\n        # Save to temp file and play\n        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as f:\n            tts.save(f.name)\n            pygame.mixer.music.load(f.name)\n            pygame.mixer.music.play()\n\n            # Wait for playback to finish\n            while pygame.mixer.music.get_busy():\n                pygame.time.Clock().tick(10)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"4-integration-launch-file",children:"4. Integration Launch File"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# launch/capstone.launch.py\n\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node, ComposableNodeContainer\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nimport os\n\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # 1. Perception (Isaac ROS)\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource('isaac_perception.launch.py')\n        ),\n\n        # 2. Navigation (Nav2)\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource('nav2_bringup.launch.py')\n        ),\n\n        # 3. Manipulation (cuMotion via MoveIt)\n        Node(\n            package='moveit_ros_move_group',\n            executable='move_group',\n            output='screen'\n        ),\n\n        # 4. VLA Components\n        Node(\n            package='my_robot_pkg',\n            executable='whisper_speech',\n            name='speech_recognition',\n            output='screen'\n        ),\n\n        Node(\n            package='my_robot_pkg',\n            executable='dialogue_manager',\n            name='dialogue_manager',\n            output='screen'\n        ),\n\n        Node(\n            package='my_robot_pkg',\n            executable='tts',\n            name='text_to_speech',\n            output='screen'\n        ),\n\n        # 5. Behavior Tree Executor\n        Node(\n            package='nav2_bt_navigator',\n            executable='bt_navigator',\n            name='bt_navigator',\n            parameters=[{\n                'default_bt_xml_filename': '/path/to/fetch_object.xml'\n            }],\n            output='screen'\n        ),\n\n        # 6. Task Coordinator\n        Node(\n            package='my_robot_pkg',\n            executable='task_executor',\n            name='task_executor',\n            output='screen'\n        ),\n    ])\n"})}),"\n",(0,i.jsx)(n.h2,{id:"testing-strategy",children:"Testing Strategy"}),"\n",(0,i.jsx)(n.h3,{id:"phase-1-simulation-testing",children:"Phase 1: Simulation Testing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# 1. Launch Isaac Sim with warehouse environment\n./isaac-sim.sh --python warehouse_scene.py\n\n# 2. Launch full stack\nros2 launch my_robot_pkg capstone.launch.py use_sim_time:=true\n\n# 3. Test individual components\nros2 topic echo /speech/text  # Verify speech recognition\nros2 topic echo /detected_objects  # Verify object detection\nros2 topic echo /robot/speech  # Verify TTS\n\n# 4. Send test command\nros2 topic pub /speech/text std_msgs/String \"data: 'Bring me the red cup'\"\n\n# 5. Verify behavior tree execution\nros2 run nav2_bt_navigator bt_navigator_status\n\n# Expected: Robot navigates, detects cup, grasps, returns\n"})}),"\n",(0,i.jsx)(n.h3,{id:"phase-2-hardware-deployment",children:"Phase 2: Hardware Deployment"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# 1. Deploy to Jetson Orin Nano (on Unitree G1)\nssh unitree@192.168.123.161\n\n# 2. Launch perception stack\nros2 launch my_robot_pkg jetson_perception.launch.py\n\n# 3. On workstation, launch high-level planning\nros2 launch my_robot_pkg dialogue_manager.launch.py\n\n# 4. Test with real speech\n# (Speak into microphone: "Hey robot, pick up the bottle")\n\n# 5. Monitor performance\njtop  # Check Jetson resource usage\nros2 topic hz /camera/image_raw  # Verify 10+ FPS\n'})}),"\n",(0,i.jsx)(n.h2,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,i.jsx)(n.h3,{id:"functionality-60-points",children:"Functionality (60 points)"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Speech recognition accuracy >90% in quiet environment"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Intent parsing correctly identifies object + location"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Navigation reaches target within 0.3m accuracy"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Object detection finds correct object >80% of time"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Grasp succeeds without dropping object"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","TTS provides clear, understandable feedback"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"robustness-20-points",children:"Robustness (20 points)"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ",'Handles "object not found" gracefully']}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Recovers from navigation failures (obstacle avoidance)"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Provides useful error messages to user"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Completes task within 5 minutes end-to-end"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"technical-quality-10-points",children:"Technical Quality (10 points)"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Code is well-documented with comments"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","ROS 2 best practices (QoS, parameters, launch files)"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Modular architecture (reusable components)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"presentation-10-points",children:"Presentation (10 points)"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Clear system architecture diagram"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Video demonstration (5-10 minutes)"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Discussion of challenges and solutions"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"common-challenges--solutions",children:"Common Challenges & Solutions"}),"\n",(0,i.jsx)(n.h3,{id:"challenge-1-speech-recognition-in-noisy-environments",children:"Challenge 1: Speech Recognition in Noisy Environments"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Add noise filtering (e.g., ",(0,i.jsx)(n.code,{children:"noisereduce"})," library) before Whisper:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import noisereduce as nr\n\n# Apply noise reduction\nreduced_audio = nr.reduce_noise(y=audio, sr=sample_rate)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"challenge-2-object-detection-false-positives",children:"Challenge 2: Object Detection False Positives"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Multi-stage verification:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"CLIP for initial detection"}),"\n",(0,i.jsx)(n.li,{children:"Depth check to confirm object distance"}),"\n",(0,i.jsx)(n.li,{children:"Secondary verification with different prompt"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"challenge-3-manipulation-collisions",children:"Challenge 3: Manipulation Collisions"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Conservative safety margins in cuMotion:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"collision_check_distance: 0.05  # 5cm buffer instead of 1cm\n"})}),"\n",(0,i.jsx)(n.h3,{id:"challenge-4-latency-in-vla-pipeline",children:"Challenge 4: Latency in VLA Pipeline"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),": Pipeline parallelism:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Start object detection while still navigating\n# (Instead of waiting for navigation to complete)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"extensions--advanced-features",children:"Extensions & Advanced Features"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-object fetch"}),': "Bring me the cup and the book"']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Conditional logic"}),': "If you find the red cup, bring it; otherwise bring the blue one"']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Learning from demonstration"}),": Show robot task once, it learns"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-robot coordination"}),": Two robots collaborate on task"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Long-term memory"}),": Robot remembers object locations across sessions"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"self-assessment-questions",children:"Self-Assessment Questions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:'How would you handle ambiguous commands like "bring me that"?'})}),"\n",(0,i.jsx)(t,{children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)("summary",{children:"Answer"}),'\nUse multimodal fusion: (1) If user is pointing (gesture recognition), use pointing direction to disambiguate. (2) If no gesture, use dialogue history (what was last discussed?). (3) Ask clarifying question: "Which object do you mean?" with TTS. (4) Use visual saliency (which object is most prominent in view?). Best practice: Always confirm before executing: "I\'ll bring the red cup. Is that correct?"']})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What is the tradeoff between using GPT-4 vs GPT-2 for intent parsing?"})}),"\n",(0,i.jsx)(t,{children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)("summary",{children:"Answer"}),"\n",(0,i.jsx)(n.strong,{children:"GPT-4"}),": Higher accuracy, better reasoning, handles complex commands. ",(0,i.jsx)(n.strong,{children:"Downsides"}),": 1-3s API latency, costs ($0.01-0.03 per request), requires internet. ",(0,i.jsx)(n.strong,{children:"GPT-2"}),": Lower accuracy, simpler reasoning. ",(0,i.jsx)(n.strong,{children:"Benefits"}),": <100ms local inference, free, works offline. ",(0,i.jsx)(n.strong,{children:"Recommendation"}),": Use GPT-2 for simple commands (detect keywords), fallback to GPT-4 for complex queries. Or fine-tune GPT-2 on robot-specific data for best of both worlds."]})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"How would you debug a behavior tree that gets stuck at a particular node?"})}),"\n",(0,i.jsx)(t,{children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)("summary",{children:"Answer"}),"\n(1) Enable BT logging: ",(0,i.jsx)(n.code,{children:"ros2 run nav2_bt_navigator bt_navigator --ros-args -p enable_debug_logging:=true"}),". (2) Visualize tree state in Groot (BT visualizer): see which node is active/failed. (3) Check node preconditions: Are all required topics publishing? Are action servers responsive? (4) Add timeout guards: If node doesn't complete in N seconds, force failure and trigger recovery. (5) Test node in isolation: Run the action server separately and send test goals."]})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why is it important to test in simulation before deploying to real hardware?"})}),"\n",(0,i.jsx)(t,{children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)("summary",{children:"Answer"}),"\n(1) ",(0,i.jsx)(n.strong,{children:"Safety"}),": Bugs in control code can damage expensive hardware ($16K+ for Unitree G1) or injure people. (2) ",(0,i.jsx)(n.strong,{children:"Iteration speed"}),": Simulation allows 10x faster testing (no robot setup, instant resets). (3) ",(0,i.jsx)(n.strong,{children:"Edge case coverage"}),": Test 100+ scenarios (narrow doorways, slippery floors, varied lighting) quickly. (4) ",(0,i.jsx)(n.strong,{children:"Cost"}),": Simulation is free; real robot testing wears components (batteries, motors). (5) ",(0,i.jsx)(n.strong,{children:"Debugging"}),": Easier to inspect internal state, pause, replay. Always validate core logic in sim before real deployment."]})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What are the ethical considerations when deploying conversational humanoid robots in homes?"})}),"\n",(0,i.jsx)(t,{children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)("summary",{children:"Answer"}),"\n(1) ",(0,i.jsx)(n.strong,{children:"Privacy"}),": Robots with cameras/microphones record private moments\u2014require clear data policies, local processing (no cloud upload). (2) ",(0,i.jsx)(n.strong,{children:"Trust"}),": Robots must be predictable; unexpected behaviors erode trust (e.g., moving while family sleeps). (3) ",(0,i.jsx)(n.strong,{children:"Accessibility"}),": Ensure speech interfaces work for accented speech, elderly users, people with disabilities. (4) ",(0,i.jsx)(n.strong,{children:"Safety"}),": Implement multiple emergency stop mechanisms; robots must never harm humans (even accidentally). (5) ",(0,i.jsx)(n.strong,{children:"Transparency"}),": Users should understand what the robot can/cannot do to avoid misplaced trust."]})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This capstone integrates everything you've learned:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2"})," provides the middleware foundation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gazebo/Isaac Sim"})," enable safe testing environments"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac ROS"})," delivers GPU-accelerated perception"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Nav2 + cuMotion"})," handle navigation and manipulation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"VLA"})," enables natural language interaction"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"congratulations",children:"Congratulations!"}),"\n",(0,i.jsx)(n.p,{children:"You've completed the Physical AI & Humanoid Robotics course. You now possess the skills to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Build ROS 2 systems from scratch"}),"\n",(0,i.jsx)(n.li,{children:"Simulate robots in Gazebo and Isaac Sim"}),"\n",(0,i.jsx)(n.li,{children:"Deploy perception stacks on NVIDIA hardware"}),"\n",(0,i.jsx)(n.li,{children:"Create vision-language-action systems"}),"\n",(0,i.jsx)(n.li,{children:"Work with real humanoid platforms"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"next-steps-in-your-journey",children:"Next Steps in Your Journey"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Contribute to open source"}),": Join ROS 2, Isaac ROS, or MoveIt communities"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Research"}),": Explore cutting-edge topics (whole-body MPC, VLA foundation models)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Industry"}),": Apply skills at robotics companies (Boston Dynamics, Tesla, Agility Robotics)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Entrepreneurship"}),": Build your own robotics startup"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Education"}),": Teach others and advance the field"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"The future of Physical AI is in your hands. Build responsibly, innovate boldly, and shape a world where humans and robots collaborate seamlessly."})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"capstone-submission-checklist",children:"Capstone Submission Checklist"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Code repository with README (GitHub/GitLab)"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","System architecture diagram"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Video demonstration (5-10 minutes)"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Technical report (5-10 pages) covering:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"System design"}),"\n",(0,i.jsx)(n.li,{children:"Implementation challenges"}),"\n",(0,i.jsx)(n.li,{children:"Evaluation results"}),"\n",(0,i.jsx)(n.li,{children:"Future improvements"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Optional: Live demo session"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Good luck with your capstone project!"})})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var s=t(6540);const i={},o=s.createContext(i);function a(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);