"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[4581],{3739:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"Module-3-ISSAC/week-09-isaac-sdk-sim","title":"Week 9: Isaac SDK and Sim Integration","description":"Master Isaac ROS packages for VSLAM, object detection, depth processing, and GPU-accelerated perception pipelines on Jetson and RTX hardware.","source":"@site/docs/Module-3-ISSAC/week-09-isaac-sdk-sim.mdx","sourceDirName":"Module-3-ISSAC","slug":"/Module-3-ISSAC/week-09-isaac-sdk-sim","permalink":"/ai-robotics-book/docs/Module-3-ISSAC/week-09-isaac-sdk-sim","draft":false,"unlisted":false,"editUrl":"https://github.com/ai-robotics/ai-robotics-book/tree/main/docs/Module-3-ISSAC/week-09-isaac-sdk-sim.mdx","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9,"title":"Week 9: Isaac SDK and Sim Integration","description":"Master Isaac ROS packages for VSLAM, object detection, depth processing, and GPU-accelerated perception pipelines on Jetson and RTX hardware."},"sidebar":"tutorialSidebar","previous":{"title":"Week 8: NVIDIA Isaac Platform Introduction","permalink":"/ai-robotics-book/docs/Module-3-ISSAC/week-08-isaac-intro"},"next":{"title":"Week 10: Advanced Isaac Features","permalink":"/ai-robotics-book/docs/Module-3-ISSAC/week-10-isaac-advanced"}}');var a=i(4848),r=i(8453);const o={sidebar_position:9,title:"Week 9: Isaac SDK and Sim Integration",description:"Master Isaac ROS packages for VSLAM, object detection, depth processing, and GPU-accelerated perception pipelines on Jetson and RTX hardware."},t="Week 9: Isaac SDK and Sim Integration",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Isaac ROS Overview",id:"isaac-ros-overview",level:2},{value:"Key Packages",id:"key-packages",level:3},{value:"Installation",id:"installation",level:3},{value:"Visual SLAM with cuVSLAM",id:"visual-slam-with-cuvslam",level:2},{value:"What is VSLAM?",id:"what-is-vslam",level:3},{value:"Launch File for cuVSLAM",id:"launch-file-for-cuvslam",level:3},{value:"Running VSLAM in Isaac Sim",id:"running-vslam-in-isaac-sim",level:3},{value:"Object Detection with TensorRT",id:"object-detection-with-tensorrt",level:2},{value:"Pre-trained Models",id:"pre-trained-models",level:3},{value:"Deploying PeopleNet",id:"deploying-peoplenet",level:3},{value:"Custom Object Detection Training",id:"custom-object-detection-training",level:3},{value:"GPU-Accelerated Image Processing",id:"gpu-accelerated-image-processing",level:2},{value:"NVIDIA VPI (Vision Programming Interface)",id:"nvidia-vpi-vision-programming-interface",level:3},{value:"Stereo Depth Estimation",id:"stereo-depth-estimation",level:3},{value:"Jetson Orin Nano Deployment",id:"jetson-orin-nano-deployment",level:2},{value:"Optimizing for Edge Hardware",id:"optimizing-for-edge-hardware",level:3},{value:"Monitoring Performance",id:"monitoring-performance",level:3},{value:"Self-Assessment Questions",id:"self-assessment-questions",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components},{Details:i}=n;return i||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"week-9-isaac-sdk-and-sim-integration",children:"Week 9: Isaac SDK and Sim Integration"})}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsxs)(n.p,{children:["NVIDIA Isaac SDK transforms raw sensor data into actionable intelligence. While Isaac Sim provides the training ground, ",(0,a.jsx)(n.strong,{children:"Isaac ROS"})," delivers production-ready perception algorithms optimized for NVIDIA GPUs. From visual SLAM to object detection, Isaac ROS packages leverage CUDA acceleration to process data 10-100x faster than CPU implementations."]}),"\n",(0,a.jsx)(n.p,{children:"This week focuses on integrating Isaac ROS with your robots\u2014both in simulation (Isaac Sim) and on real hardware (Jetson Orin Nano). You'll deploy pre-trained AI models, implement VSLAM for localization, and build GPU-accelerated perception pipelines."}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Install"})," Isaac ROS packages on Ubuntu 22.04 with ROS 2 Humble"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Implement"})," visual SLAM using Isaac ROS VSLAM (cuVSLAM)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Deploy"})," pre-trained object detection models (PeopleNet, DOPE)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Understand"})," GPU-accelerated image processing with NVIDIA VPI"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Optimize"})," perception pipelines for Jetson Orin Nano deployment"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"isaac-ros-overview",children:"Isaac ROS Overview"}),"\n",(0,a.jsx)(n.h3,{id:"key-packages",children:"Key Packages"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Package"}),(0,a.jsx)(n.th,{children:"Function"}),(0,a.jsx)(n.th,{children:"Hardware Acceleration"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"isaac_ros_visual_slam"})}),(0,a.jsx)(n.td,{children:"Visual-inertial SLAM"}),(0,a.jsx)(n.td,{children:"CUDA + TensorRT"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"isaac_ros_dnn_inference"})}),(0,a.jsx)(n.td,{children:"Deep learning inference"}),(0,a.jsx)(n.td,{children:"TensorRT"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"isaac_ros_image_proc"})}),(0,a.jsx)(n.td,{children:"Image preprocessing"}),(0,a.jsx)(n.td,{children:"VPI (Vision Programming Interface)"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"isaac_ros_depth_segmentation"})}),(0,a.jsx)(n.td,{children:"Depth-based segmentation"}),(0,a.jsx)(n.td,{children:"CUDA"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"isaac_ros_object_detection"})}),(0,a.jsx)(n.td,{children:"2D/3D object detection"}),(0,a.jsx)(n.td,{children:"TensorRT"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"isaac_ros_apriltag"})}),(0,a.jsx)(n.td,{children:"AprilTag detection"}),(0,a.jsx)(n.td,{children:"CUDA"})]})]})]}),"\n",(0,a.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Install Isaac ROS packages\nsudo apt install ros-humble-isaac-ros-visual-slam \\\n                 ros-humble-isaac-ros-dnn-inference \\\n                 ros-humble-isaac-ros-image-proc\n\n# Verify CUDA is available\nnvidia-smi  # Should show GPU\n\n# Test Isaac ROS\nros2 run isaac_ros_visual_slam isaac_ros_visual_slam  # Should launch without errors\n"})}),"\n",(0,a.jsx)(n.h2,{id:"visual-slam-with-cuvslam",children:"Visual SLAM with cuVSLAM"}),"\n",(0,a.jsx)(n.h3,{id:"what-is-vslam",children:"What is VSLAM?"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)("span",{className:"highlight-purple",children:(0,a.jsx)(n.strong,{children:"Visual SLAM (Simultaneous Localization and Mapping)"})})," uses camera and IMU data to:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Build a map of the environment"}),"\n",(0,a.jsx)(n.li,{children:"Localize the robot within that map"}),"\n",(0,a.jsx)(n.li,{children:"All without GPS or external landmarks"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"cuVSLAM"})," is NVIDIA's GPU-accelerated VSLAM, running 10x faster than CPU alternatives."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:'flowchart LR\n    Camera["Stereo Camera<br/>(Raw Images)"]\n    IMU["IMU<br/>(Acceleration, Gyro)"]\n    VPI["VPI<br/>(Image Preprocessing)"]\n    cuVSLAM["cuVSLAM<br/>(Feature Tracking)"]\n    TensorRT["TensorRT<br/>(DNN Inference)"]\n    Map["Map<br/>(3D Landmarks)"]\n    Pose["Robot Pose<br/>(TF: map\u2192base_link)"]\n\n    Camera --\x3e VPI\n    VPI --\x3e cuVSLAM\n    IMU --\x3e cuVSLAM\n    cuVSLAM --\x3e Map\n    cuVSLAM --\x3e Pose\n    Camera --\x3e TensorRT\n    TensorRT --\x3e cuVSLAM\n\n    style cuVSLAM fill:#a855f7,stroke:#9333ea,stroke-width:2px,color:#fff\n    style TensorRT fill:#ec4899,stroke:#db2777,stroke-width:2px,color:#fff\n    style VPI fill:#06b6d4,stroke:#0891b2,stroke-width:2px,color:#fff\n'})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Diagram:"})," Isaac ROS perception pipeline showing stereo camera and IMU data flowing through VPI preprocessing, cuVSLAM for localization, and TensorRT for object detection."]}),"\n",(0,a.jsx)(n.h3,{id:"launch-file-for-cuvslam",children:"Launch File for cuVSLAM"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# launch/vslam.launch.py\n\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node, ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\n\ndef generate_launch_description():\n    return LaunchDescription([\n        ComposableNodeContainer(\n            name='vslam_container',\n            namespace='',\n            package='rclcpp_components',\n            executable='component_container',\n            composable_node_descriptions=[\n                # Visual SLAM node\n                ComposableNode(\n                    package='isaac_ros_visual_slam',\n                    plugin='nvidia::isaac_ros::visual_slam::VisualSlamNode',\n                    name='visual_slam',\n                    parameters=[{\n                        'enable_rectified_pose': True,\n                        'denoise_input_images': False,\n                        'rectified_images': True,\n                        'enable_debug_mode': False,\n                        'debug_dump_path': '/tmp/cuvslam',\n                        'enable_slam_visualization': True,\n                        'enable_landmarks_view': True,\n                        'enable_observations_view': True,\n                        'map_frame': 'map',\n                        'odom_frame': 'odom',\n                        'base_frame': 'base_link',\n                        'input_camera_frame': 'camera_link',\n                        'enable_imu_fusion': True,\n                    }],\n                    remappings=[\n                        ('/visual_slam/image_0', '/camera/left/image_raw'),\n                        ('/visual_slam/camera_info_0', '/camera/left/camera_info'),\n                        ('/visual_slam/image_1', '/camera/right/image_raw'),\n                        ('/visual_slam/camera_info_1', '/camera/right/camera_info'),\n                        ('/visual_slam/imu', '/imu/data'),\n                    ]\n                ),\n            ],\n            output='screen'\n        ),\n    ])\n"})}),"\n",(0,a.jsx)(n.h3,{id:"running-vslam-in-isaac-sim",children:"Running VSLAM in Isaac Sim"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Start Isaac Sim with stereo camera robot\n./isaac-sim.sh --python stereo_robot.py\n\n# Terminal 2: Launch VSLAM\nros2 launch my_robot_pkg vslam.launch.py\n\n# Terminal 3: Visualize in RViz\nrviz2\n\n# In RViz:\n# Add -> TF (see map -> odom -> base_link frames)\n# Add -> PointCloud2 (/visual_slam/vis/landmarks) - see SLAM map points\n# Add -> Odometry (/visual_slam/tracking/odometry) - see robot path\n"})}),"\n",(0,a.jsx)(n.h2,{id:"object-detection-with-tensorrt",children:"Object Detection with TensorRT"}),"\n",(0,a.jsx)(n.h3,{id:"pre-trained-models",children:"Pre-trained Models"}),"\n",(0,a.jsx)(n.p,{children:"Isaac ROS provides models for common tasks:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"PeopleNet"}),": Detect people in images"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"DOPE (Deep Object Pose Estimation)"}),": Detect 3D poses of known objects"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"TrafficCamNet"}),": Detect vehicles, pedestrians, road signs"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"deploying-peoplenet",children:"Deploying PeopleNet"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# launch/peopl enet_detection.launch.py\n\nfrom launch import LaunchDescription\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\n\ndef generate_launch_description():\n    return LaunchDescription([\n        ComposableNodeContainer(\n            name='detection_container',\n            namespace='',\n            package='rclcpp_components',\n            executable='component_container',\n            composable_node_descriptions=[\n                # TensorRT inference node\n                ComposableNode(\n                    package='isaac_ros_dnn_inference',\n                    plugin='nvidia::isaac_ros::dnn_inference::TensorRTInferenceNode',\n                    name='tensorrt_inference',\n                    parameters=[{\n                        'model_file_path': '/workspaces/isaac_ros-dev/models/peoplenet/1/model.onnx',\n                        'engine_file_path': '/tmp/peoplenet.plan',\n                        'input_tensor_names': ['input_tensor'],\n                        'input_binding_names': ['input_1'],\n                        'output_tensor_names': ['output_cov', 'output_bbox'],\n                        'output_binding_names': ['output_cov/Sigmoid', 'output_bbox/BiasAdd'],\n                        'verbose': False,\n                        'force_engine_update': False\n                    }],\n                    remappings=[\n                        ('/tensor_pub', '/image_tensor'),\n                    ]\n                ),\n\n                # Detection decoder\n                ComposableNode(\n                    package='isaac_ros_detectnet',\n                    plugin='nvidia::isaac_ros::detectnet::DetectNetDecoderNode',\n                    name='detectnet_decoder',\n                    parameters=[{\n                        'label_list': ['person'],\n                        'confidence_threshold': 0.35,\n                    }],\n                ),\n            ],\n            output='screen'\n        ),\n    ])\n"})}),"\n",(0,a.jsx)(n.h3,{id:"custom-object-detection-training",children:"Custom Object Detection Training"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# 1. Generate synthetic data in Isaac Sim (Week 8)\n# 2. Export to COCO format\n# 3. Train DetectNet model using TAO Toolkit\n\n# Install TAO Toolkit\ndocker pull nvcr.io/nvidia/tao/tao-toolkit:4.0.0-tf2.11.0\n\n# Train (simplified - see NVIDIA TAO docs for full pipeline)\ntao detectnet_v2 train -e /workspace/spec.txt \\\n                        -r /workspace/output \\\n                        -k $API_KEY\n\n# Export to TensorRT\ntao detectnet_v2 export -m /workspace/model.hdf5 \\\n                         -o /workspace/model.onnx \\\n                         -k $API_KEY\n"})}),"\n",(0,a.jsx)(n.h2,{id:"gpu-accelerated-image-processing",children:"GPU-Accelerated Image Processing"}),"\n",(0,a.jsx)(n.h3,{id:"nvidia-vpi-vision-programming-interface",children:"NVIDIA VPI (Vision Programming Interface)"}),"\n",(0,a.jsx)(n.p,{children:"VPI offloads image processing to GPU/VIC (Vision Image Compositor):"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from isaac_ros_image_proc import ImageFlipNode, ImageFormatConverterNode\n\n# In your launch file:\nComposableNode(\n    package='isaac_ros_image_proc',\n    plugin='nvidia::isaac_ros::image_proc::ImageFlipNode',\n    name='image_flip',\n    parameters=[{\n        'flip_mode': 'HORIZONTAL',  # HORIZONTAL, VERTICAL, BOTH\n    }],\n    remappings=[\n        ('/image', '/camera/image_raw'),\n        ('/flipped_image', '/camera/image_flipped'),\n    ]\n),\n"})}),"\n",(0,a.jsx)(n.h3,{id:"stereo-depth-estimation",children:"Stereo Depth Estimation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"ComposableNode(\n    package='isaac_ros_stereo_image_proc',\n    plugin='nvidia::isaac_ros::stereo_image_proc::DisparityNode',\n    name='disparity',\n    parameters=[{\n        'backends': 'CUDA',  # Use GPU acceleration\n        'max_disparity': 64.0,\n    }],\n),\n"})}),"\n",(0,a.jsx)(n.h2,{id:"jetson-orin-nano-deployment",children:"Jetson Orin Nano Deployment"}),"\n",(0,a.jsx)(n.h3,{id:"optimizing-for-edge-hardware",children:"Optimizing for Edge Hardware"}),"\n",(0,a.jsx)(n.p,{children:"Jetson Orin Nano has limited power (15W mode) compared to RTX workstations. Optimizations:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reduce Resolution"}),": 640x480 instead of 1920x1080"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Lower Frame Rate"}),": 10 FPS for detection (30 FPS not needed)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"TensorRT INT8"}),": Use 8-bit quantization (4x faster than FP16)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Disable Debug Outputs"}),": Reduce CPU overhead"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Jetson-optimized parameters\nparameters=[{\n    'image_width': 640,\n    'image_height': 480,\n    'frame_rate': 10,\n    'use_tensorrt_int8': True,  # Enable INT8 quantization\n    'dla_core': 0,  # Use Deep Learning Accelerator\n}],\n"})}),"\n",(0,a.jsx)(n.h3,{id:"monitoring-performance",children:"Monitoring Performance"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Check Jetson stats\nsudo jetson_stats  # or 'jtop'\n\n# Expected on Orin Nano (15W):\n# - PeopleNet: 15-20 FPS at 640x480\n# - VSLAM: 30 FPS stereo processing\n# - Power: 10-12W\n"})}),"\n",(0,a.jsx)(n.h2,{id:"self-assessment-questions",children:"Self-Assessment Questions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Why is cuVSLAM faster than CPU-based SLAM (e.g., ORB-SLAM)?"})}),"\n",(0,a.jsx)(i,{children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)("summary",{children:"Answer"}),"\ncuVSLAM parallelizes feature detection, matching, and bundle adjustment on thousands of GPU cores, whereas CPU-based SLAM is limited to sequential processing on 4-16 cores. GPUs excel at the massively parallel operations in SLAM: extracting features from every pixel, matching thousands of feature points, and solving large optimization problems. This results in 10-20x speedups, enabling real-time SLAM on high-resolution images (1920x1080) where CPU implementations would drop to 1-5 FPS."]})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"What is TensorRT, and why is it critical for deploying AI models on Jetson?"})}),"\n",(0,a.jsx)(i,{children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)("summary",{children:"Answer"}),"\nTensorRT is NVIDIA's inference optimizer that converts trained models (ONNX, TensorFlow) into highly optimized engines for NVIDIA hardware. It applies: (1) ",(0,a.jsx)(n.strong,{children:"Layer fusion"})," (combine operations to reduce memory reads), (2) ",(0,a.jsx)(n.strong,{children:"Precision calibration"})," (FP32 \u2192 FP16 \u2192 INT8 quantization), (3) ",(0,a.jsx)(n.strong,{children:"Kernel tuning"})," (optimize for specific GPU architecture). On Jetson, this means 3-10x faster inference and 50% less power consumption compared to unoptimized models\u2014critical for battery-powered robots where every watt matters."]})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"How does stereo depth estimation differ from structured light (RealSense)?"})}),"\n",(0,a.jsx)(i,{children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)("summary",{children:"Answer"}),"\n",(0,a.jsx)(n.strong,{children:"Stereo"})," uses two cameras (like human eyes) to compute depth via triangulation\u2014finding corresponding points in left/right images. ",(0,a.jsx)(n.strong,{children:"Structured light"})," projects an infrared pattern and measures distortion. Stereo: works outdoors (sunlight doesn't interfere), longer range, no projector needed, but requires textured surfaces for matching. Structured light: works on textureless surfaces, higher accuracy at close range (<5m), but fails outdoors due to IR interference. Isaac ROS stereo is GPU-accelerated for real-time performance (30+ FPS)."]})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Why use composable nodes instead of separate node executables in Isaac ROS?"})}),"\n",(0,a.jsx)(i,{children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)("summary",{children:"Answer"}),"\nComposable nodes run in a single process with shared memory (zero-copy communication), whereas separate executables communicate via DDS (serialization/deserialization overhead). For high-bandwidth data like images (1920x1080 RGB = 6MB per frame at 30 FPS = 180 MB/s), avoiding copies is critical. Composable nodes reduce latency by 50-80% and CPU usage significantly. This is essential on Jetson where CPU is limited and every millisecond counts for real-time perception."]})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"What is INT8 quantization, and what tradeoff does it involve?"})}),"\n",(0,a.jsx)(i,{children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)("summary",{children:"Answer"}),"\nINT8 quantization converts model weights from 32-bit floating point (FP32) to 8-bit integers, reducing model size by 75% and speeding up inference 2-4x. The tradeoff is reduced precision\u2014INT8 can represent fewer distinct values (256 vs 4 billion for FP32), potentially decreasing accuracy by 0.5-2%. TensorRT minimizes this via calibration (finding optimal scale factors) using a calibration dataset. For most perception tasks (detection, segmentation), accuracy loss is negligible (<1%), making INT8 ideal for edge deployment."]})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS"})," provides GPU-accelerated perception packages"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"cuVSLAM"})," enables real-time visual SLAM on NVIDIA hardware"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"TensorRT"})," optimizes AI models for edge deployment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"VPI"})," accelerates image processing on GPU/VIC"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Jetson Orin Nano"})," runs production perception pipelines at 10-30 FPS"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsxs)(n.p,{children:["Week 10 explores ",(0,a.jsx)(n.strong,{children:"Advanced Isaac Features"}),": Nav2 integration for autonomous navigation, cuMotion for collision-free path planning, and multi-robot simulation at scale."]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>t});var s=i(6540);const a={},r=s.createContext(a);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);