"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[5024],{3223:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Module-4-VLA/week-11-humanoid-development","title":"Week 11: Humanoid Robot Development","description":"Deploy perception, locomotion, and manipulation stacks on real humanoid platforms like Unitree G1, with Jetson Orin Nano edge compute.","source":"@site/docs/Module-4-VLA/week-11-humanoid-development.mdx","sourceDirName":"Module-4-VLA","slug":"/Module-4-VLA/week-11-humanoid-development","permalink":"/ai-robotics-book/docs/Module-4-VLA/week-11-humanoid-development","draft":false,"unlisted":false,"editUrl":"https://github.com/ai-robotics/ai-robotics-book/tree/main/docs/Module-4-VLA/week-11-humanoid-development.mdx","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"sidebar_position":11,"title":"Week 11: Humanoid Robot Development","description":"Deploy perception, locomotion, and manipulation stacks on real humanoid platforms like Unitree G1, with Jetson Orin Nano edge compute."},"sidebar":"tutorialSidebar","previous":{"title":"Week 10: Advanced Isaac Features","permalink":"/ai-robotics-book/docs/Module-3-ISSAC/week-10-isaac-advanced"},"next":{"title":"Week 11: Humanoid Robot Development","permalink":"/ai-robotics-book/docs/Module-4-VLA/week-11-humanoid-development"}}');var i=o(4848),s=o(8453);const r={sidebar_position:11,title:"Week 11: Humanoid Robot Development",description:"Deploy perception, locomotion, and manipulation stacks on real humanoid platforms like Unitree G1, with Jetson Orin Nano edge compute."},l="Week 11: Humanoid Robot Development",a={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Unitree G1 Platform Overview",id:"unitree-g1-platform-overview",level:2},{value:"Hardware Specifications",id:"hardware-specifications",level:3},{value:"Software Stack",id:"software-stack",level:3},{value:"Setting Up Jetson Orin Nano",id:"setting-up-jetson-orin-nano",level:2},{value:"Installation",id:"installation",level:3},{value:"Network Configuration",id:"network-configuration",level:3},{value:"Whole-Body Control",id:"whole-body-control",level:2},{value:"What is Whole-Body Control?",id:"what-is-whole-body-control",level:3},{value:"Control Architecture",id:"control-architecture",level:3},{value:"Code Example: Simple Balance Controller",id:"code-example-simple-balance-controller",level:3},{value:"Locomotion Patterns",id:"locomotion-patterns",level:2},{value:"Gait Generation",id:"gait-generation",level:3},{value:"Terrain Adaptation",id:"terrain-adaptation",level:3},{value:"Safety Systems",id:"safety-systems",level:2},{value:"Emergency Stop",id:"emergency-stop",level:3},{value:"Collision Detection",id:"collision-detection",level:3},{value:"Deploying Perception on Jetson",id:"deploying-perception-on-jetson",level:2},{value:"Optimized Launch File",id:"optimized-launch-file",level:3},{value:"Performance Monitoring",id:"performance-monitoring",level:3},{value:"Self-Assessment Questions",id:"self-assessment-questions",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components},{Details:o}=n;return o||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"week-11-humanoid-robot-development",children:"Week 11: Humanoid Robot Development"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(n.p,{children:["Everything converges this week: ROS 2, Isaac perception, and real hardware. You'll work with humanoid platforms like ",(0,i.jsx)(n.strong,{children:"Unitree G1"}),"\u201423+ degrees of freedom, bipedal locomotion, dexterous manipulation\u2014deploying the perception and control stacks you've built throughout this course."]}),"\n",(0,i.jsx)(n.p,{children:"Humanoid robots represent the frontier of Physical AI: they navigate human environments, use human tools, and interact naturally with people. From warehouse logistics to elder care, humanoids are poised to transform how robots integrate into society."}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Understand"})," humanoid robot architectures (sensing, computing, actuation)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Deploy"})," Isaac ROS on Jetson Orin Nano for onboard perception"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implement"})," whole-body control for coordinated locomotion and manipulation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Configure"})," safety systems (collision detection, emergency stop)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integrate"})," IMU-based balance control for stable bipedal walking"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"unitree-g1-platform-overview",children:"Unitree G1 Platform Overview"}),"\n",(0,i.jsx)(n.h3,{id:"hardware-specifications",children:"Hardware Specifications"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Component"}),(0,i.jsx)(n.th,{children:"Specification"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Height"})}),(0,i.jsx)(n.td,{children:"130 cm (adjustable)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Weight"})}),(0,i.jsx)(n.td,{children:"35 kg"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"DoF"})}),(0,i.jsx)(n.td,{children:"23-43 (configurable)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Actuators"})}),(0,i.jsx)(n.td,{children:"Quasi-direct-drive motors (high torque density)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Compute"})}),(0,i.jsx)(n.td,{children:"NVIDIA Jetson Orin (32GB) + STM32 microcontrollers"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Sensors"})}),(0,i.jsx)(n.td,{children:"3D LIDAR, Depth cameras (RealSense), IMU, force/torque sensors"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Battery"})}),(0,i.jsx)(n.td,{children:"Li-ion, 2-4 hour runtime"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Communication"})}),(0,i.jsx)(n.td,{children:"Ethernet (control), WiFi (telemetry)"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"software-stack",children:"Software Stack"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    High-Level Planning (ROS 2)          \u2502 \u2190 Your code\n\u2502  (Navigation, Task Planning, VLA)       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    Perception (Isaac ROS on Jetson)     \u2502 \u2190 Week 9\n\u2502  (VSLAM, Object Detection, Depth)       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    Whole-Body Controller                \u2502 \u2190 This week\n\u2502  (Balance, Locomotion, Manipulation)    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    Motor Controllers (STM32 Real-Time)  \u2502 \u2190 Firmware\n\u2502  (PID, Torque Control, Safety)          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h2,{id:"setting-up-jetson-orin-nano",children:"Setting Up Jetson Orin Nano"}),"\n",(0,i.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# 1. Flash Jetson with Ubuntu 22.04 + ROS 2 Humble\n# Download JetPack 6.0 from NVIDIA Developer\n\n# 2. Install Isaac ROS\nsudo apt install ros-humble-isaac-ros-visual-slam \\\n                 ros-humble-isaac-ros-dnn-inference\n\n# 3. Install Unitree SDK\ngit clone https://github.com/unitreerobotics/unitree_ros2.git ~/ros2_ws/src/\ncd ~/ros2_ws\ncolcon build --packages-select unitree_ros2\n\n# 4. Test connection to robot\nros2 topic list  # Should show /joint_states, /imu, /camera/* if robot is powered on\n"})}),"\n",(0,i.jsx)(n.h3,{id:"network-configuration",children:"Network Configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Jetson IP: 192.168.123.161 (default)\n# Robot control board: 192.168.123.10\n\n# Test connection\nping 192.168.123.10\n\n# SSH into Jetson from workstation\nssh unitree@192.168.123.161  # Password: unitree (change immediately!)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"whole-body-control",children:"Whole-Body Control"}),"\n",(0,i.jsx)(n.h3,{id:"what-is-whole-body-control",children:"What is Whole-Body Control?"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)("span",{className:"highlight-purple",children:(0,i.jsx)(n.strong,{children:"Whole-body control"})})," coordinates all joints simultaneously to achieve:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Balance (keep center of mass over support polygon)"}),"\n",(0,i.jsx)(n.li,{children:"Locomotion (walking, turning, climbing stairs)"}),"\n",(0,i.jsx)(n.li,{children:"Manipulation (reaching, grasping while balancing)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Unlike fixed-base manipulators, humanoids must continuously adjust posture to avoid falling."}),"\n",(0,i.jsx)(n.h3,{id:"control-architecture",children:"Control Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:'flowchart TB\n    Task["Desired Task<br/>(Reach, Walk, Grasp)"]\n    IK["Inverse Kinematics<br/>(Joint Angles)"]\n    WBC["Whole-Body Controller<br/>(Balance + Task)"]\n    Torque["Joint Torque<br/>Commands"]\n    Motors["Motors<br/>(23 DoF)"]\n    IMU["IMU Feedback<br/>(Balance)"]\n    Joints["Joint Encoders<br/>(Position)"]\n\n    Task --\x3e IK\n    IK --\x3e WBC\n    WBC --\x3e Torque\n    Torque --\x3e Motors\n    Motors --\x3e Joints\n    Motors --\x3e IMU\n    IMU --\x3e WBC\n    Joints --\x3e WBC\n\n    style WBC fill:#a855f7,stroke:#9333ea,stroke-width:2px,color:#fff\n    style Motors fill:#ec4899,stroke:#db2777,stroke-width:2px,color:#fff\n    style IMU fill:#06b6d4,stroke:#0891b2,stroke-width:2px,color:#fff\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Diagram:"})," Humanoid whole-body control architecture with feedback loops from IMU and joint encoders to maintain balance while executing tasks."]}),"\n",(0,i.jsx)(n.h3,{id:"code-example-simple-balance-controller",children:"Code Example: Simple Balance Controller"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu, JointState\nfrom std_msgs.msg import Float64MultiArray\nimport numpy as np\n\n\nclass BalanceController(Node):\n    def __init__(self):\n        super().__init__(\'balance_controller\')\n\n        # Subscribe to IMU (balance feedback)\n        self.imu_sub = self.create_subscription(\n            Imu, \'/imu/data\', self.imu_callback, 10\n        )\n\n        # Subscribe to joint states\n        self.joint_sub = self.create_subscription(\n            JointState, \'/joint_states\', self.joint_callback, 10\n        )\n\n        # Publish joint commands\n        self.cmd_pub = self.create_publisher(\n            Float64MultiArray, \'/joint_commands\', 10\n        )\n\n        # Control gains\n        self.kp_pitch = 50.0  # Proportional gain for pitch correction\n        self.kd_pitch = 10.0  # Derivative gain\n\n        self.pitch = 0.0\n        self.pitch_rate = 0.0\n\n        # Control loop at 100 Hz\n        self.create_timer(0.01, self.control_loop)\n\n    def imu_callback(self, msg):\n        """Extract pitch angle from IMU orientation"""\n        # Convert quaternion to Euler angles\n        # (Simplified - use tf_transformations in production)\n        self.pitch = 2 * np.arcsin(msg.orientation.y)  # Approximate\n\n        # Angular velocity (pitch rate)\n        self.pitch_rate = msg.angular_velocity.y\n\n    def joint_callback(self, msg):\n        # Store current joint positions for safety checks\n        pass\n\n    def control_loop(self):\n        """PD controller to maintain upright posture"""\n\n        # Desired pitch: 0 radians (upright)\n        pitch_error = 0.0 - self.pitch\n\n        # PD control law\n        ankle_torque = (self.kp_pitch * pitch_error +\n                        self.kd_pitch * (-self.pitch_rate))\n\n        # Limit torque (safety)\n        ankle_torque = np.clip(ankle_torque, -50.0, 50.0)  # Nm\n\n        # Publish command\n        cmd = Float64MultiArray()\n        cmd.data = [0.0] * 23  # 23 joints (example)\n        cmd.data[12] = ankle_torque  # Left ankle pitch\n        cmd.data[18] = ankle_torque  # Right ankle pitch\n\n        self.cmd_pub.publish(cmd)\n\n\ndef main():\n    rclpy.init()\n    controller = BalanceController()\n    rclpy.spin(controller)\n    controller.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsx)(n.p,{children:"This is a simplified example. Production balance controllers use model predictive control (MPC), center of mass (CoM) tracking, and zero-moment point (ZMP) stabilization. Never deploy untested controllers\u2014robots can fall and damage themselves!"})}),"\n",(0,i.jsx)(n.h2,{id:"locomotion-patterns",children:"Locomotion Patterns"}),"\n",(0,i.jsx)(n.h3,{id:"gait-generation",children:"Gait Generation"}),"\n",(0,i.jsx)(n.p,{children:"Bipedal walking requires coordinated foot placement:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class GaitGenerator:\n    def __init__(self):\n        self.step_length = 0.3  # 30 cm steps\n        self.step_height = 0.05  # 5 cm foot clearance\n        self.step_period = 1.0  # 1 second per step\n        self.phase = 0.0  # Current gait phase [0, 1]\n\n    def update(self, dt):\n        """Update gait phase"""\n        self.phase += dt / self.step_period\n        if self.phase > 1.0:\n            self.phase -= 1.0\n\n    def get_foot_position(self, is_left_foot):\n        """Compute foot position in gait cycle"""\n\n        if self.phase < 0.5:\n            # Left foot swing, right foot support\n            if is_left_foot:\n                # Swing trajectory (parabolic arc)\n                x = self.step_length * (self.phase * 2)\n                z = self.step_height * np.sin(np.pi * self.phase * 2)\n            else:\n                # Support foot (stationary)\n                x = 0.0\n                z = 0.0\n        else:\n            # Right foot swing, left foot support\n            if not is_left_foot:\n                x = self.step_length * ((self.phase - 0.5) * 2)\n                z = self.step_height * np.sin(np.pi * (self.phase - 0.5) * 2)\n            else:\n                x = 0.0\n                z = 0.0\n\n        return [x, 0.0, z]\n'})}),"\n",(0,i.jsx)(n.h3,{id:"terrain-adaptation",children:"Terrain Adaptation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def adapt_gait_to_terrain(self, lidar_scan):\n    """Adjust step length/height based on terrain"""\n\n    # Analyze LIDAR to detect obstacles\n    min_distance = np.min(lidar_scan.ranges)\n\n    if min_distance < 0.5:  # Obstacle within 50cm\n        self.step_length = 0.2  # Shorter steps\n        self.step_height = 0.1  # Higher clearance\n    elif min_distance < 1.0:  # Caution zone\n        self.step_length = 0.25\n        self.step_height = 0.07\n    else:  # Clear path\n        self.step_length = 0.3\n        self.step_height = 0.05\n'})}),"\n",(0,i.jsx)(n.h2,{id:"safety-systems",children:"Safety Systems"}),"\n",(0,i.jsx)(n.h3,{id:"emergency-stop",children:"Emergency Stop"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class SafetyMonitor(Node):\n    def __init__(self):\n        super().__init__(\'safety_monitor\')\n\n        # Subscribe to IMU for fall detection\n        self.imu_sub = self.create_subscription(\n            Imu, \'/imu/data\', self.check_fall, 10\n        )\n\n        # Emergency stop service\n        self.estop_srv = self.create_service(\n            Trigger, \'/emergency_stop\', self.emergency_stop_callback\n        )\n\n        self.estop_triggered = False\n\n    def check_fall(self, msg):\n        """Detect if robot is falling"""\n\n        # Check if pitch or roll exceeds threshold\n        pitch = 2 * np.arcsin(msg.orientation.y)\n        roll = 2 * np.arcsin(msg.orientation.x)\n\n        if abs(pitch) > 0.5 or abs(roll) > 0.5:  # ~30 degrees\n            self.get_logger().error(\'FALL DETECTED! Triggering E-STOP\')\n            self.trigger_estop()\n\n    def trigger_estop(self):\n        """Immediately halt all motors"""\n        if not self.estop_triggered:\n            self.estop_triggered = True\n\n            # Publish zero velocity command\n            zero_cmd = Float64MultiArray()\n            zero_cmd.data = [0.0] * 23\n            # (publish to motor controller)\n\n            # Log event\n            self.get_logger().fatal(\'E-STOP ACTIVATED\')\n\n    def emergency_stop_callback(self, request, response):\n        self.trigger_estop()\n        response.success = True\n        response.message = "Emergency stop activated"\n        return response\n'})}),"\n",(0,i.jsx)(n.h3,{id:"collision-detection",children:"Collision Detection"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def check_self_collision(self, joint_states):\n    """Detect if robot limbs are colliding"""\n\n    # Use forward kinematics to compute link positions\n    link_positions = self.compute_fk(joint_states)\n\n    # Check distance between non-adjacent links\n    for i in range(len(link_positions)):\n        for j in range(i + 2, len(link_positions)):\n            distance = np.linalg.norm(\n                link_positions[i] - link_positions[j]\n            )\n\n            if distance < 0.05:  # 5cm minimum clearance\n                self.get_logger().warn(\n                    f\'Self-collision detected: Link {i} and {j}\'\n                )\n                return True\n\n    return False\n'})}),"\n",(0,i.jsx)(n.h2,{id:"deploying-perception-on-jetson",children:"Deploying Perception on Jetson"}),"\n",(0,i.jsx)(n.h3,{id:"optimized-launch-file",children:"Optimized Launch File"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# launch/jetson_perception.launch.py\n\nfrom launch import LaunchDescription\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\n\ndef generate_launch_description():\n    return LaunchDescription([\n        ComposableNodeContainer(\n            name='perception_container',\n            namespace='',\n            package='rclcpp_components',\n            executable='component_container',\n            composable_node_descriptions=[\n                # VSLAM for localization\n                ComposableNode(\n                    package='isaac_ros_visual_slam',\n                    plugin='nvidia::isaac_ros::visual_slam::VisualSlamNode',\n                    name='visual_slam',\n                    parameters=[{\n                        'enable_imu_fusion': True,\n                        'enable_slam_visualization': False,  # Reduce CPU load\n                    }],\n                ),\n\n                # Object detection (INT8 for efficiency)\n                ComposableNode(\n                    package='isaac_ros_dnn_inference',\n                    plugin='nvidia::isaac_ros::dnn_inference::TensorRTInferenceNode',\n                    name='object_detector',\n                    parameters=[{\n                        'use_tensorrt_int8': True,  # 4x faster on Jetson\n                        'dla_core': 0,  # Use DLA (Deep Learning Accelerator)\n                    }],\n                ),\n            ],\n            output='screen'\n        ),\n    ])\n"})}),"\n",(0,i.jsx)(n.h3,{id:"performance-monitoring",children:"Performance Monitoring"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Monitor Jetson stats in real-time\nsudo jtop\n\n# Expected performance on Orin Nano (15W mode):\n# - VSLAM: 20-30 FPS (stereo 640x480)\n# - Object detection: 15 FPS (640x480 INT8)\n# - Power: 10-12W\n# - Temperature: 50-60\xb0C\n"})}),"\n",(0,i.jsx)(n.h2,{id:"self-assessment-questions",children:"Self-Assessment Questions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why do humanoid robots require whole-body control instead of independent joint control?"})}),"\n",(0,i.jsx)(o,{children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)("summary",{children:"Answer"}),"\nHumanoids are underactuated systems (cannot directly control center of mass position) and must maintain balance via coordinated joint movements. Independent joint control doesn't account for coupling\u2014moving an arm shifts the center of mass, requiring leg adjustments to prevent falling. Whole-body control solves a unified optimization problem: achieve task objectives (e.g., reach) while satisfying balance constraints (keep CoM over support polygon). This coordination is critical for dynamic tasks like walking while carrying objects."]})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"How does IMU-based fall detection differ from vision-based fall detection?"})}),"\n",(0,i.jsx)(o,{children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)("summary",{children:"Answer"}),"\nIMU-based detection measures orientation and acceleration directly at high frequency (100-1000 Hz), providing immediate feedback for balance control. It works regardless of lighting and has low latency. Vision-based detection (analyzing camera images) is slower (30 FPS typical), computationally expensive, and can fail in poor lighting. However, vision can predict falls by detecting uneven terrain ahead, whereas IMU only reacts after the robot starts tilting. Best practice: Use IMU for reactive control and vision for predictive planning."]})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why is INT8 quantization especially important for humanoid robots?"})}),"\n",(0,i.jsx)(o,{children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)("summary",{children:"Answer"}),"\nHumanoids are battery-powered with strict power budgets (10-15W for compute). INT8 reduces inference time by 3-4x and power consumption by ~50% compared to FP16, extending battery life from 2 hours to 3-4 hours. The accuracy tradeoff (typically <1% for well-calibrated models) is acceptable for most perception tasks. On Jetson Orin Nano, INT8 enables running VSLAM + object detection + depth processing simultaneously\u2014impossible at FP32 due to thermal and power limits."]})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What is the zero-moment point (ZMP), and why is it critical for bipedal stability?"})}),"\n",(0,i.jsx)(o,{children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)("summary",{children:"Answer"}),"\nThe ZMP is the point on the ground where the sum of horizontal moments equals zero. For stable walking, the ZMP must lie within the support polygon (footprint on the ground). If ZMP moves outside, the robot begins to tip. Balance controllers compute foot placements and CoM trajectories to keep ZMP within bounds. This is a fundamental constraint in bipedal locomotion\u2014violated ZMP leads to falls. ZMP is used in model predictive control to plan stable walking gaits."]})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"How would you tune gait parameters for a humanoid climbing stairs versus walking on flat ground?"})}),"\n",(0,i.jsx)(o,{children:(0,i.jsxs)(n.p,{children:[(0,i.jsx)("summary",{children:"Answer"}),"\n",(0,i.jsx)(n.strong,{children:"Stairs"}),": (1) Reduce step length (15-20cm vs 30cm), (2) Increase step height (15-20cm vs 5cm) to clear stair risers, (3) Slow down step period (1.5-2s vs 1s) for stability, (4) Increase ankle torque limits for pushing off higher steps, (5) Add vision-based stair detection to align foot placement with stair edges. ",(0,i.jsx)(n.strong,{children:"Flat ground"}),": Optimize for speed and efficiency with longer, faster steps and minimal foot clearance. Stair climbing requires conservative, deliberate movements; flat ground allows aggressive gaits."]})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Unitree G1"})," exemplifies modern humanoid platforms (23+ DoF, Jetson Orin, LIDAR/cameras)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Whole-body control"})," coordinates all joints for balance and task execution"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gait generation"})," produces stable bipedal walking patterns"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety systems"})," (E-STOP, fall detection, self-collision) prevent damage"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Jetson deployment"})," requires optimizations (INT8, reduced resolution, DLA)"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(n.p,{children:["Week 12 explores ",(0,i.jsx)(n.strong,{children:"Multi-modal Interactions"}),": integrating vision, language, and action for natural human-robot collaboration. You'll implement vision-language models for object understanding and gesture recognition for intuitive control."]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>l});var t=o(6540);const i={},s=t.createContext(i);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);